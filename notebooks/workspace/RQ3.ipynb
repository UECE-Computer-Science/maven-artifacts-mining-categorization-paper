{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad912a-e4e4-4b6a-883a-08a6f948f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import basic_auth\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import time\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import display\n",
    "\n",
    "# Configuração do Neo4J\n",
    "uri = \"neo4j://138.2.240.156:7688\"\n",
    "auth = basic_auth(\"neo4j\", \"Password1\")\n",
    "db = \"neo4j\"\n",
    "\n",
    "# Conexão ao Neo4j\n",
    "driver = GraphDatabase.driver(uri, auth=auth)\n",
    "\n",
    "# Function to check connection and list databases\n",
    "def test_connection():\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"SHOW DATABASES YIELD name\")\n",
    "            print(\"Connection to Neo4j successful!\\nAvailable databases:\")\n",
    "            for record in result:\n",
    "                print(record['name'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Neo4j: {e}\")\n",
    "\n",
    "def run_query(query):\n",
    "    with driver.session(database=db) as session:\n",
    "        return result\n",
    "\n",
    "def run_query_graph(query):\n",
    "    with driver.session(database=db) as session:\n",
    "        result = session.run(query)\n",
    "        graph = result.graph()\n",
    "        return GraphWidget(graph=graph)\n",
    "\n",
    "def run_query_data(query):\n",
    "    \"\"\"\n",
    "    Executes a query against the database and returns a DataFrame.\n",
    "    Records query response time.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    with driver.session(database=db) as session:\n",
    "        result = session.run(query)\n",
    "        data = [record.data() for record in result]\n",
    "        end_time = time.time()\n",
    "        duration_ms = (end_time - start_time) * 1000\n",
    "        print(f\"Response time: {duration_ms:.2f} ms\")\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "def run_query_data_graph(query):\n",
    "    with driver.session(database=db) as session:\n",
    "        result = session.run(query)\n",
    "        data = [record.data() for record in result]\n",
    "        graph = result.graph()\n",
    "        graph_widget = GraphWidget(graph=graph)\n",
    "        display(graph_widget)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "def save_to_csv_incrementally(df, filename):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a CSV file, adding to the end if the file already exists.\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, mode='a', header=not pd.io.common.file_exists(filename), index=False)\n",
    "\n",
    "# Test the initial connection\n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ae61e-1c6f-4177-b8e9-b6820855652a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def run_query(query):\n",
    "    with driver.session(database=db) as session:\n",
    "        result = session.run(query)\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "\n",
    "input_csv = 'no_categorized.csv'  \n",
    "output_folder = 'output_dependencies/'  \n",
    "\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\n",
    "ids = pd.read_csv(input_csv)['ID'].tolist()\n",
    "\n",
    "\n",
    "batch_size = 100000  \n",
    "current_batch = []   \n",
    "batch_count = 1     \n",
    "\n",
    "\n",
    "for idx, artifact_id in enumerate(ids, start=1):\n",
    "    \n",
    "    query = f\"MATCH (a:Artifact {{id: '{artifact_id}'}})<-[:dependency]-(dependentArtifact) RETURN DISTINCT dependentArtifact.id AS DependencyArtifactID\"\n",
    "    dependencies = run_query(query)\n",
    "    \n",
    "    \n",
    "    if dependencies:\n",
    "        print(f\"ID: {artifact_id} - find dependencies: {len(dependencies)}\")\n",
    "        \n",
    "        for dep in dependencies:\n",
    "            current_batch.append({'ArtifactID': artifact_id, 'DependencyID': dep['DependencyArtifactID']})\n",
    "    else:\n",
    "        print(f\"ID: {artifact_id} - any find dependencies.\")\n",
    "\n",
    "\n",
    "    if len(current_batch) >= batch_size or idx == len(ids):\n",
    "     \n",
    "        output_csv = os.path.join(output_folder, f'batch_{batch_count}_dependencies.csv')\n",
    "        pd.DataFrame(current_batch).to_csv(output_csv, index=False)\n",
    "        print(f\"Lote {batch_count} salvo em: {output_csv}\")\n",
    "        \n",
    "    \n",
    "        current_batch = []\n",
    "        batch_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c0156-0183-4281-bb5f-40c765d31dfb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_folder = 'output_dependencies/'\n",
    "output_file = 'consolidated_dependencies.csv'\n",
    "\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "\n",
    "csv_files = [f for f in sorted(os.listdir(input_folder)) if f.endswith('.csv')]\n",
    "\n",
    "total_files = len(csv_files)\n",
    "\n",
    "\n",
    "for idx, filename in enumerate(csv_files, start=1):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    print(f\"Lendo arquivo: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)  \n",
    "    \n",
    "    # Barra de progresso a cada 25%\n",
    "    progress = (idx / total_files) * 100\n",
    "    if progress >= 25 and progress < 50 and idx % (total_files // 4) == 0:\n",
    "        print(\"25% ok...\")\n",
    "    elif progress >= 50 and progress < 75 and idx % (total_files // 2) == 0:\n",
    "        print(\"50% ok...\")\n",
    "    elif progress >= 75 and idx % (3 * total_files // 4) == 0:\n",
    "        print(\"75% ok...\")\n",
    "\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "print(f\"File generated successfully: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3468f2e-848a-4c2d-adab-e61e329cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_csv = 'consolidated_dependencies.csv'\n",
    "output_csv = 'cleaned_consolidated_dependencies.csv'  \n",
    "\n",
    "\n",
    "chunk_size = 500000  \n",
    "chunks = []\n",
    "\n",
    "\n",
    "for chunk in pd.read_csv(input_csv, chunksize=chunk_size):\n",
    "    \n",
    "    chunk['ArtifactID'] = chunk['ArtifactID'].apply(lambda x: ':'.join(x.split(':')[:2]))\n",
    "    chunk['DependencyID'] = chunk['DependencyID'].apply(lambda x: ':'.join(x.split(':')[:2]))\n",
    "   \n",
    "    chunk = chunk.drop_duplicates()\n",
    "    chunks.append(chunk)\n",
    "\n",
    "\n",
    "cleaned_df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "\n",
    "cleaned_df.to_csv(output_csv, index=False)\n",
    "print(f\"File generated clean save as: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e289291-d220-4fba-b42b-89639a8c9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "csv1_file = 'categorized.csv'\n",
    "csv2_file = 'consolidate_deps_uniquis.csv'\n",
    "output_file = 'TAGS_noCategorizedsNewVersion.csv'\n",
    "\n",
    "\n",
    "csv1 = pd.read_csv(csv1_file)\n",
    "csv2 = pd.read_csv(csv2_file)\n",
    "\n",
    "\n",
    "id_to_tag = dict(zip(csv1['ID'], csv1['TAG']))\n",
    "\n",
    "filtered_csv2 = csv2[csv2['DependencyID'].isin(id_to_tag.keys())].copy()\n",
    "\n",
    "filtered_csv2.loc[:, 'TAG'] = filtered_csv2['DependencyID'].map(id_to_tag)\n",
    "\n",
    "filtered_csv2.to_csv(output_file, index=False)\n",
    "print(f\"File generated successfully: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ffd7a-8d64-4bd8-919b-12cf81c93554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
